{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "EPSILON = 0.9\n",
    "GAMMA = 0.9\n",
    "LR = 0.01\n",
    "MEMORY_CAPACITY = 2000\n",
    "Q_NETWORK_ITERATION = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPISODES = 40\n",
    "env = gym.make('MountainCar-v0')# 3个动作，（向左，向右，不动）\n",
    "env = env.unwrapped\n",
    "NUM_STATES = env.observation_space.shape[0] # 2\n",
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 30)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(30, NUM_ACTIONS)#三哥动作\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn():\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net() #构建两个网络，eval主网络，算Qfunction，第二个记忆中的网络target（提供标签）\n",
    "        #两个网络结构相同，但是里面参数不同\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES *2 +2))\n",
    "        # state, action ,reward and next state\n",
    "        self.memory_counter = 0\n",
    "        self.learn_counter = 0\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), LR)#优化器对主网络去做\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "\n",
    "    def store_trans(self, state, action, reward, next_state):\n",
    "        if self.memory_counter % 500 ==0:\n",
    "            print(\"The experience pool collects {} time experience\".format(self.memory_counter))\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        trans = np.hstack((state, [action], [reward], next_state))#记录一条数据\n",
    "        self.memory[index,] = trans\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # notation that the function return the action's index nor the real action\n",
    "        # EPSILON\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state) ,0)\n",
    "        if np.random.randn() <= EPSILON:#探索 增加随机变量增加学习空间 如果小于EPSILION 按部就班的走\n",
    "            action_value = self.eval_net.forward(state)# 得到各个action的得分\n",
    "            action= torch.max(action_value,dim=1)[1].data.np() # 找最大的那个action\n",
    "            action = action[0] #get the action index\n",
    "        else:\n",
    "            action = np.random.randint(0,NUM_ACTIONS)#大于0.9随便随机一个action的\n",
    "        return action\n",
    "\n",
    "    def plot(self, ax, x):\n",
    "        ax.cla()\n",
    "        ax.set_xlabel(\"episode\")\n",
    "        ax.set_ylabel(\"total reward\")\n",
    "        ax.plot(x, 'b-')\n",
    "        plt.pause(0.000000000000001)\n",
    "\n",
    "    def learn(self):\n",
    "        # learn 100 times then the target network update\n",
    "        if self.learn_counter % Q_NETWORK_ITERATION ==0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())#学了100次之后target才更新（直接加载eval的权重）\n",
    "        self.learn_counter+=1\n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)#在保存的数据里，获取一个batch数据\n",
    "        batch_memory = self.memory[sample_index, :]#通过index取出实际数据\n",
    "        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n",
    "        #note that the action must be a int\n",
    "        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n",
    "        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n",
    "        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(batch_state).gather(1, batch_action)#得到当前Q(s,a)上面一个值，因为直接算出来acton\n",
    "        q_next = self.target_net(batch_next_state).detach()#得到Q(s',a')，下面选max#\n",
    "        q_target = batch_reward + GAMMA*q_next.max(1)[0].view(BATCH_SIZE, 1)#公式\n",
    "\n",
    "        loss = self.loss(q_eval, q_target)#差异越小越好\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    net = Dqn()\n",
    "    print(\"The DQN is collecting experience...\")\n",
    "    step_counter_list = []\n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset() #reset 一下\n",
    "        step_counter = 0\n",
    "        while True:\n",
    "            step_counter +=1\n",
    "            env.render()\n",
    "            action = net.choose_action(state)#先从网络得到action\n",
    "            next_state, reward, done, info = env.step(action)#在游戏里玩一步\n",
    "            reward = reward * 100 if reward >0 else reward * 5 #作者放大奖励了\n",
    "            net.store_trans(state, action, reward, next_state)#记录当前这组数据当（当前状态，当前动作，当前奖励，下一个状态）\n",
    "\n",
    "            if net.memory_counter >= MEMORY_CAPACITY: # 攒够数据一起学 2000 走了2000步，个一起学\n",
    "                net.learn()#更新模型\n",
    "                if done:\n",
    "                    print(\"episode {}, the reward is {}\".format(episode, round(reward, 3)))\n",
    "            if done:\n",
    "                step_counter_list.append(step_counter)\n",
    "                net.plot(net.ax, step_counter_list)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
